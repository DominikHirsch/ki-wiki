---
title: Geschichte der künstlichen Intelligenz
---

# Geschichte der künstlichen Intelligenz

## 1. Einleitung

Die Geschichte der künstlichen Intelligenz (KI) beginnt Mitte des 20. Jahrhunderts und ist geprägt von Pionieren, technologischen Durchbrüchen, Rücksetzern („KI-Winter“) und der rasanten Entwicklung moderner, datengetriebener Methoden.

---

## 2. Ursprünge und erste Konzepte

1. **Alan Turing (1950):**  
   - Werk: „Computing Machinery and Intelligence“  
   - Einführung des **Turing-Tests**: Maßstab, ob eine Maschine menschliches Verhalten imitieren kann.

2. **Dartmouth Conference (1956):**  
   - Markiert die offizielle Geburtsstunde der KI als Forschungsgebiet.  
   - Teilnehmer: John McCarthy, Marvin Minsky, Claude Shannon, etc.  
   - Ziel: Maschinen sollen „lernen“ und „denken“ können.

---

## 3. Symbolische KI & Expertensysteme (1956–1970er)

- **Symbolische KI („Good Old-Fashioned AI“)**  
  - Arbeitet mit Logik, Regeln und Wissensdatenbanken.  
  - Beispiel: „General Problem Solver“ (Newell & Simon, 1957).

- **Expertensysteme (1970er–1980er)**  
  - Wissensbasierte Systeme, die Fachwissen in Form von Wenn–Dann-Regeln nutzen.  
  - Beispiel: **MYCIN** (1972) – Diagnosesystem für bakterielle Infektionen.

---

## 4. Erster KI-Winter (1974–1980)

- **Ursachen:**  
  - Überschätzte Versprechen, geringe Rechenleistung, unzureichende Daten.  
  - Finanzierungskürzungen, Forschungsergebnisse blieben hinter Erwartungen zurück.

- **Folgen:**  
  - Rückgang der Forschungsförderung in den USA und UK.  
  - Fokusverschiebung hin zu spezialisierteren, kleineren Projekten.

---

## 5. Zweite Blüte & Zweiter KI-Winter (1980er–1990er)

1. **Expertensystem-Ära (1980er):**  
   - Kommerzielle Expertensysteme (z. B. XCON von Digital Equipment Corporation).  
   - Große Investitionen, aber hoher Entwicklungsaufwand.

2. **Zweiter KI-Winter (Ende 1980er):**  
   - Kosten/Nutzen-Relation von Expertensystemen nicht mehr gegeben.  
   - Striktere Finanzierung und Einbruch der KI-Hype-Phase.

---

## 6. Aufstieg des Machine Learning (1990er–2000er)

- **Maschinelles Lernen (ML):**  
  - Fokus auf statistische Verfahren, neuronale Netze und Datenanalyse.  
  - Beispiel: **Backpropagation** (Rumelhart, Hinton & Williams, 1986) – Durchbruch für Trainings tiefer Netze.

- **Support Vector Machines (SVM), Random Forests, etc.**  
  - Einführung in der akademischen und industriellen Praxis.

---

## 7. Deep Learning & Big Data (2010–heute)

1. **Deep Learning:**  
   - Tiefen neuronale Netze (Deep Neural Networks)  
   - Beispiel: **AlexNet** (2012) – wichtige Architektur für Bildklassifikation.

2. **Transformer-Architektur (2017):**  
   - Vaswani et al.: „Attention is All You Need“  
   - Grundlage für große Sprachmodelle (LLMs) wie BERT, GPT, etc.

3. **Große Sprachmodelle (LLMs):**  
   - **GPT-2/3/4** (OpenAI), **BERT** (Google), **Claude** (Anthropic), **Gemini** (Google)  
   - Fähigkeiten: Textgenerierung, Übersetzung, Frageantworten, etc.

---

## 8. Aktueller Stand der KI-Forschung

| **Bereich**                | **Beschreibung**                                                                           |
|----------------------------|-------------------------------------------------------------------------------------------|
| Sprachmodelle (LLMs)       | GPT-4, Bard, Claude – State-of-the-Art in Textgenerierung und -verstehen.                 |
| Computer Vision            | CNNs, Transformer-Modelle für Bild- und Objekterkennung (z. B. YOLO, Vision Transformers). |
| Reinforcement Learning     | Anwendungen in Robotik, Spiele (AlphaGo, OpenAI Five).                                    |
| KI-Ethik & Fairness        | Forschung zu Bias, Transparenz, Erklärbarkeit (XAI).                                      |
| Multimodale Modelle        | Kombination von Text, Bild, Audio (z. B. CLIP, DALL·E).                                   |

---

## 9. Meilensteintabelle (Zeitstrahl)

| **Jahr** | **Meilenstein**                                                                 |
|----------|---------------------------------------------------------------------------------|
| 1950     | Alan Turing: „Computing Machinery and Intelligence“                             |
| 1956     | Dartmouth Conference: Begriff „Künstliche Intelligenz“ etabliert                 |
| 1972     | Expertensystem MYCIN für medizinische Diagnosen                                  |
| 1980     | Aufstieg kommerzieller Expertensysteme (XCON, MYCIN, etc.)                       |
| 1987–1993| Zweiter KI-Winter: Rückgang der Finanzierung und Forschung                       |
| 1986     | Backpropagation-Algorithmus (Rumelhart, Hinton & Williams)                        |
| 1997     | Deep Blue schlägt Schachweltmeister Garry Kasparov                               |
| 2012     | AlexNet gewinnt ImageNet-Wettbewerb, Durchbruch für Deep Learning                |
| 2017     | Einführung der Transformer-Architektur (Vaswani et al.)                          |
| 2020     | GPT-3 veröffentlicht, mehr als 175 Mrd. Parameter                                 |
| 2023     | GPT-4 und weitere multimodale Modelle eröffnen neue Anwendungen in Text & Bild    |

---

## 10. Weiterführende Ressourcen

- **Buch:** “Artificial Intelligence: A Modern Approach” (Russell & Norvig)  
- **Artikel:** Vaswani, A. et al. (2017). “Attention is All You Need.”  
- **Online:** [AI Timeline | AI100@Stanford](https://ai100.stanford.edu/monitoring-ai/ai-timeline)  
- **Website:** [DeepAI History of AI](https://deepai.org/history-of-ai)

